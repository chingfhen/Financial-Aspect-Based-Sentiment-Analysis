{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590289bb",
   "metadata": {},
   "source": [
    "# ABOUT:\n",
    "- Multi Aspect Multi Sentiment (MAMS) Dataset is an food review ABSA dataset where samples have multiple sentiments and aspects\n",
    "- MAMS has 2 dataset types: ATSA and ACSA. This research focuses on ATSA\n",
    "- this notebook:\n",
    "    1. parses MAMS xml file into a dataframe\n",
    "    2. converts into a training dataset\n",
    "    3. save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78c6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from bs4.element import Tag\n",
    "import jsonlines\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334efc57",
   "metadata": {},
   "source": [
    "#### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2ebe549",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\local\\data\\NLPCC 2020 Shared Task 2 Guideline  Multi-Aspect-based Multi Sentiment Analysis (MAMS)\\Dataset_MAMS\\ATSA\\train.xml\"\n",
    "with open(path, 'r') as f:\n",
    "    data = f.read()\n",
    "    data = BeautifulSoup(data, \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313cafb",
   "metadata": {},
   "source": [
    "### 1. Parse ATSA XML dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "988c5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ATSA_sentence(sentence, remove_newlines = True):     # takes a sentence TAG, returns the relevant units of information\n",
    "    text = sentence.text\n",
    "    if remove_newlines:\n",
    "        text = text.replace('\\n',\"\")\n",
    "    aspectTerms = tuple((a['from'],a['polarity'],a['term'],a['to']) for a in sentence.aspectTerms if isinstance(a,Tag))\n",
    "    return {\"text\":text,\"aspectTerms\":aspectTerms}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65c3e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import ClassLabel, Sequence\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "def show_elements(dataset, randomize = True, num_samples = 10):\n",
    "    \n",
    "    if isinstance(dataset,pd.DataFrame):                  # if DataFrame \n",
    "        if randomize:                                          # if random> shuffle\n",
    "            dataset = dataset.sample(frac=1)\n",
    "        display(HTML(dataset.iloc[:num_samples].to_html()))             # take first n rows\n",
    "    \n",
    "    else:                                                    # if not DataFrame\n",
    "        if randomize:                                           # if random> shuffle\n",
    "            dataset = dataset.shuffle()   \n",
    "        dataset = pd.DataFrame(dataset.select(range(num_samples)))   # convert first n rows to dataframe\n",
    "        display(HTML(dataset.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "413c5383",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4297/4297 [00:00<00:00, 36898.80it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for s in tqdm.tqdm(data.find_all(\"sentence\")):\n",
    "    parsed_sentence = parse_ATSA_sentence(s)\n",
    "    dataset.append(parsed_sentence)\n",
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8990959",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspectTerms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food and amazing prices make up for it.</td>\n",
       "      <td>((4, negative, decor, 9), (42, positive, food, 46), (59, positive, prices, 65))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when tables opened up, the manager sat another party before us.</td>\n",
       "      <td>((5, neutral, tables, 11), (27, negative, manager, 34))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Though the menu includes some unorthodox offerings (a peanut butter roll, for instance), the classics are pure and great--we've never had better sushi anywhere, including Japan.</td>\n",
       "      <td>((11, neutral, menu, 15), (54, negative, peanut butter roll, 72), (93, positive, classics, 101), (145, positive, sushi, 150))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>service is good although a bit in your face, we were asked every five mins if food was ok, but better that than being ignored.</td>\n",
       "      <td>((0, positive, service, 7), (78, neutral, food, 82))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PS- I just went for brunch on Saturday and the eggs served with onions and rosemary were amazing.</td>\n",
       "      <td>((20, neutral, brunch, 26), (47, positive, eggs served with onions, 70))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>they didn't have to change anything about the menu except add a leg of chicken seperatley and the guy mumbled very rudely that I had already ordered and I should've decided earlier.</td>\n",
       "      <td>((46, neutral, menu, 50), (98, negative, guy, 101))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The server came to us and was sooo hot, he went over the menu and specials with us.</td>\n",
       "      <td>((4, positive, server, 10), (57, neutral, menu, 61))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Food The best surprises on the El Salvadorean menu are the appetizers.</td>\n",
       "      <td>((4, neutral, Food, 8), (63, positive, appetizers, 73))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12/24/03  Dinner was ok, service was so- so,the worst part was the hostess - we made reservations a month before Christmas Eve for three people, you would think the table would be large enough for all three of us.</td>\n",
       "      <td>((10, positive, Dinner, 16), (25, neutral, service, 32), (67, negative, hostess, 74))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Still, after all the fuss, the food makes you forget about the wait.</td>\n",
       "      <td>((31, positive, food, 35), (63, neutral, wait, 67))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_elements(dataset,randomize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f389035d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4297"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0929dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.028857342331859"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average number of sentiments per sample - Fi_ATSA\n",
    "def get_num_unique_sentiments(label):\n",
    "    return len(set(l[1] for l in label))\n",
    "num_unique_sentiments = dataset.aspectTerms.apply(lambda label: get_num_unique_sentiments(label))\n",
    "sum(num_unique_sentiments)/len(num_unique_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b56312",
   "metadata": {},
   "source": [
    "### 2. convert into training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f34ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mams_dataset(dataset):\n",
    "    def _generate_sample(text, aspectTerms):\n",
    "        output = []\n",
    "        label_mapping = {\"negative\":0,\"neutral\":1,\"positive\":2}\n",
    "        for AT in aspectTerms:\n",
    "            output.append({\"text\":text, \n",
    "                           \"span\": (int(AT[0]), int(AT[-1])),\n",
    "                           \"label\":label_mapping[AT[1]]})\n",
    "        return output\n",
    "    samples = []\n",
    "    for i in range(len(dataset)):\n",
    "        samples.extend(_generate_sample(dataset.loc[i,\"text\"],dataset.loc[i,\"aspectTerms\"]))\n",
    "    return pd.DataFrame(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ae6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_mams_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44493c4b",
   "metadata": {},
   "source": [
    "### 3. save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1885995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),\"data\\\\MAMS\\\\train\\\\mams_atsa_train.pkl\")\n",
    "dataset.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55090aa5",
   "metadata": {},
   "source": [
    "### Input for SATSA \n",
    "- text: a sentence e.g \"The decor is not special at all but their..\"\n",
    "- span: the span of the aspect e.g (4, 9)\t\n",
    "- label: the sentiment of the aspect e.g 0\n",
    "\n",
    "note: label_mappings = {\"negative\":0,\"neutral\":1,\"positive\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1c5104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>(4, 9)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>(42, 46)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The decor is not special at all but their food...</td>\n",
       "      <td>(59, 65)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>(5, 11)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when tables opened up, the manager sat another...</td>\n",
       "      <td>(27, 34)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      span  label\n",
       "0  The decor is not special at all but their food...    (4, 9)      0\n",
       "1  The decor is not special at all but their food...  (42, 46)      2\n",
       "2  The decor is not special at all but their food...  (59, 65)      2\n",
       "3  when tables opened up, the manager sat another...   (5, 11)      1\n",
       "4  when tables opened up, the manager sat another...  (27, 34)      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0e8972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The decor is not special at all but their food and amazing prices make up for it.',\n",
       "  'span': (4, 9),\n",
       "  'label': 0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_dict(\"records\")[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1573399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11186"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27090d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7c1181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAMS Dataset\n",
      "Total number of unique aspects: 2586\n",
      "Size of training set: 4297\n",
      "Average number of aspects per sample: 2.6032115429369327\n",
      "\n",
      "Fi_ATSA Dataset (my own)\n",
      "Total number of unique aspects: 383\n",
      "Size of training set: 413\n",
      "Average number of aspects per sample: 3.49636803874092\n",
      "\n",
      "FiQA Dataset\n",
      "Total number of unique aspects: 526\n",
      "Size of training set: 1078\n",
      "Average number of aspects per sample: 1.0556586270871986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "aspect_set = set()\n",
    "path = os.path.join(os.getcwd(),\"data\\\\MAMS\\\\train\\\\mams_atsa_train.pkl\")\n",
    "df = pd.read_pickle(path)\n",
    "for i in range(len(df)):\n",
    "    span = df.loc[i,\"span\"]\n",
    "    aspect = df.loc[i,\"text\"][span[0]:span[1]]\n",
    "    aspect_set.add(aspect)\n",
    "counts = Counter(df.text).values()\n",
    "print(\"MAMS Dataset\")\n",
    "print(\"Total number of unique aspects:\", len(aspect_set))\n",
    "print(\"Size of training set:\", len(counts))\n",
    "print(\"Average number of aspects per sample:\", sum(counts)/len(counts),end = \"\\n\\n\")\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "aspect_set = set()\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\URECA--Financial-Aspect-Based-Sentiment-Analysis\\data\\Fi_ATSA\\train\\Fi_ATSA_train.pkl\"\n",
    "df = pd.read_pickle(path)\n",
    "for i in range(len(df)):\n",
    "    span = df.loc[i,\"span\"]\n",
    "    aspect = df.loc[i,\"text\"][span[0]:span[1]]\n",
    "    aspect_set.add(aspect)\n",
    "counts = Counter(df.text).values()\n",
    "print(\"Fi_ATSA Dataset (my own)\")\n",
    "print(\"Total number of unique aspects:\", len(aspect_set))\n",
    "print(\"Size of training set:\", len(counts))\n",
    "print(\"Average number of aspects per sample:\", sum(counts)/len(counts),end = \"\\n\\n\")\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "aspect_set = set()\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\URECA--Financial-Aspect-Based-Sentiment-Analysis\\data\\FiQA\\train\\FiQA_train.pkl\"\n",
    "df = pd.read_pickle(path)\n",
    "for i in range(len(df)):\n",
    "    span = df.loc[i,\"span\"]\n",
    "    aspect = df.loc[i,\"text\"][span[0]:span[1]]\n",
    "    aspect_set.add(aspect)\n",
    "counts = Counter(df.text).values()\n",
    "print(\"FiQA Dataset\")\n",
    "print(\"Total number of unique aspects:\", len(aspect_set))\n",
    "print(\"Size of training set:\", len(counts))\n",
    "print(\"Average number of aspects per sample:\", sum(counts)/len(counts),end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a106609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FiQA Dataset\n",
      "Total number of unique aspects: 526\n",
      "Size of training set: 1078\n",
      "Average number of aspects per sample: 1.0556586270871986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "aspect_set = set()\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\URECA--Financial-Aspect-Based-Sentiment-Analysis\\data\\FiQA\\train\\FiQA_train.pkl\"\n",
    "df = pd.read_pickle(path)\n",
    "for i in range(len(df)):\n",
    "    span = df.loc[i,\"span\"]\n",
    "    aspect = df.loc[i,\"text\"][span[0]:span[1]]\n",
    "    aspect_set.add(aspect)\n",
    "counts = Counter(df.text).values()\n",
    "print(\"FiQA Dataset\")\n",
    "print(\"Total number of unique aspects:\", len(aspect_set))\n",
    "print(\"Size of training set:\", len(counts))\n",
    "print(\"Average number of aspects per sample:\", sum(counts)/len(counts),end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e342e4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Royal Mail chairman Donald Brydon set to step ...</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stakes High for AstraZeneca Heart Drug Facing ...</td>\n",
       "      <td>(16, 27)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UPDATE 1Dairy Crest loses a third of Morrisons...</td>\n",
       "      <td>(37, 46)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insight hires Avivas David Hillier for multias...</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Primark racks up a happy Christmas after stron...</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>Facebook FB received a Buy rating from Wells F...</td>\n",
       "      <td>(9, 11)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>TSLA Wish had my puts back but see if we can f...</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>Citrix Systems Inc CTXS Position Increased by ...</td>\n",
       "      <td>(19, 23)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Notable gainers among liquid option names this...</td>\n",
       "      <td>(65, 66)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>Is Facebooks user engagement falling FB.</td>\n",
       "      <td>(37, 39)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1138 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      span  label\n",
       "0     Royal Mail chairman Donald Brydon set to step ...   (0, 10)      0\n",
       "1     Stakes High for AstraZeneca Heart Drug Facing ...  (16, 27)      0\n",
       "2     UPDATE 1Dairy Crest loses a third of Morrisons...  (37, 46)      1\n",
       "3     Insight hires Avivas David Hillier for multias...    (0, 7)      1\n",
       "4     Primark racks up a happy Christmas after stron...    (0, 7)      2\n",
       "...                                                 ...       ...    ...\n",
       "1133  Facebook FB received a Buy rating from Wells F...   (9, 11)      2\n",
       "1134  TSLA Wish had my puts back but see if we can f...    (0, 4)      0\n",
       "1135  Citrix Systems Inc CTXS Position Increased by ...  (19, 23)      2\n",
       "1136  Notable gainers among liquid option names this...  (65, 66)      2\n",
       "1137           Is Facebooks user engagement falling FB.  (37, 39)      0\n",
       "\n",
       "[1138 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = {}\n",
    "for text in df.text:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c49584ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001A1D660A310>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"text\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16717ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be7ceb",
   "metadata": {},
   "source": [
    "### perform same steps for dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\local\\data\\NLPCC 2020 Shared Task 2 Guideline  Multi-Aspect-based Multi Sentiment Analysis (MAMS)\\Dataset_MAMS\\ATSA\\dev.xml\"\n",
    "with open(path, 'r') as f:\n",
    "    data = f.read()\n",
    "    data = BeautifulSoup(data, \"xml\")\n",
    "   \n",
    " # parse\n",
    "dataset = []\n",
    "for s in tqdm.tqdm(data.find_all(\"sentence\")):\n",
    "    parsed_sentence = parse_ATSA_sentence(s)\n",
    "    dataset.append(parsed_sentence)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "# process\n",
    "dataset = process_mams_dataset(dataset)\n",
    "\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\local\\data\\mams_atsa_dev.pkl\"\n",
    "dataset.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6396e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc09b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056e3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82854378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272e630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5730c7fe",
   "metadata": {},
   "source": [
    "# ACSA below is not the focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ad6a0",
   "metadata": {},
   "source": [
    "#### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\local\\data\\NLPCC 2020 Shared Task 2 Guideline  Multi-Aspect-based Multi Sentiment Analysis (MAMS)\\Dataset_MAMS\\ACSA\\train.xml\"\n",
    "with open(path, 'r') as f:\n",
    "    data = f.read()\n",
    "    data = BeautifulSoup(data, \"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ACSA_sentence(sentence, remove_newlines = True):     # takes a sentence TAG, returns the relevant units of information\n",
    "    text = sentence.text\n",
    "    if remove_newlines:\n",
    "        text = text.replace('\\n',\"\")\n",
    "    aspectCategories = tuple((c['category'],c['polarity']) for c in sentence.aspectCategories if isinstance(c,Tag))\n",
    "        \n",
    "    return {\"text\":text,\"aspectCategories\":aspectCategories}      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3333b48",
   "metadata": {},
   "source": [
    "# Parse ACSA XML dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for s in tqdm.tqdm(data.find_all(\"sentence\")):\n",
    "    parsed_sentence = parse_ACSA_sentence(s)\n",
    "    dataset.append(parsed_sentence)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a699e",
   "metadata": {},
   "source": [
    "### save dataset\n",
    "- as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\local\\data\\NLPCC 2020 Shared Task 2 Guideline  Multi-Aspect-based Multi Sentiment Analysis (MAMS)\\Dataset_MAMS\\ACSA\\train.csv\"\n",
    "dataset.to_csv(path,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f62026",
   "metadata": {},
   "source": [
    "## preprocess and save dev datasets too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94569ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\local\\data\\NLPCC 2020 Shared Task 2 Guideline  Multi-Aspect-based Multi Sentiment Analysis (MAMS)\\Dataset_MAMS\\ACSA\\dev.xml\"\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    data = f.read()\n",
    "    data = BeautifulSoup(data, \"xml\")\n",
    "    \n",
    "dataset = []\n",
    "for s in tqdm.tqdm(data.find_all(\"sentence\")):\n",
    "    parsed_sentence = parse_ACSA_sentence(s)\n",
    "    dataset.append(parsed_sentence)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\URECA - Aspect Based Sentiment Analysis\\local\\data\\NLPCC 2020 Shared Task 2 Guideline  Multi-Aspect-based Multi Sentiment Analysis (MAMS)\\Dataset_MAMS\\ACSA\\dev.csv\"\n",
    "dataset.to_csv(path,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983a35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
